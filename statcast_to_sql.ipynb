{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "840b5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import datetime\n",
    "import sqlite3 \n",
    "import pybaseball as pyb #Pybaseball is where we will be pulling data from\n",
    "#!pip install nbconvert\n",
    "#!pip install pyppeteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f5b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get today's date as the latest day to pull data ( pybaseball will see tomorrow's date and only pull up to the most recent data)\n",
    "today = datetime.date.today() + datetime.timedelta(days=1) \n",
    "#turn the output of the date to a string to be able to pass through the statcast function\n",
    "today_str = today.strftime('%Y-%m-%d')  \n",
    "#This start_dt will be the starting date of spring training for the 2023 season \n",
    "start_dt_od = '2023-03-30'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f4a2957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 133/133 [00:30<00:00,  4.42it/s]\n"
     ]
    }
   ],
   "source": [
    "#Get all data leading from Opening Day to the current date from pybaseball's statcast function \n",
    "data = pyb.statcast(start_dt=start_dt_od, end_dt=today_str, team=None, verbose=True, parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb101f",
   "metadata": {},
   "source": [
    "# This to add barrel to a batted ball event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e9ae6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_barrel(df):\n",
    "    # Fill missing values in 'launch_angle' and 'exit_velo' columns with 0\n",
    "    df['launch_angle'] = df['launch_angle'].fillna(0)\n",
    "    df['exit_velo'] = df['exit_velo'].fillna(0)\n",
    "\n",
    "    # Create the 'barrel' column based on the specified conditions\n",
    "    df['barrel'] = (\n",
    "        (df['launch_angle'] <= 50) &\n",
    "        (df['exit_velo'] >= 98) &\n",
    "        (df['exit_velo'] * 1.5 - df['launch_angle'] >= 117) &\n",
    "        (df['exit_velo'] + df['launch_angle'] >= 124)\n",
    "    )\n",
    "\n",
    "    # Convert boolean values to integers (0 for False, 1 for True)\n",
    "    df['barrel'] = df['barrel'].astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7db214",
   "metadata": {},
   "source": [
    "# These are to map release height and side buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "266607fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_height_bucket(value):\n",
    "    if pd.notna(value):\n",
    "        if value < 1:\n",
    "            return '<1'\n",
    "        elif 1 <= value <= 1.5:\n",
    "            return '1.0-1.5'\n",
    "        elif 1.5 < value <= 2:\n",
    "            return '1.5-2.0'\n",
    "        elif 2 < value <= 2.5:\n",
    "            return '2.0-2.5'\n",
    "        elif 2.5 < value <= 3:\n",
    "            return '2.5-3.0'\n",
    "        elif 3 < value <= 3.5:\n",
    "            return '3.0-3.5'\n",
    "        elif 3.5 < value <= 4:\n",
    "            return '3.5-4.0'\n",
    "        elif 4 < value <= 4.5:\n",
    "            return '4.0-4.5'\n",
    "        elif 4.5 < value <= 5:\n",
    "            return '4.5-5.0'\n",
    "        elif 5 < value <= 5.5:\n",
    "            return '5.0-5.5'\n",
    "        elif 5.5 < value <= 6:\n",
    "            return '5.5-6.0'\n",
    "        elif 6 < value <= 6.5:\n",
    "            return '6.0-6.5'\n",
    "        elif 6.5 < value <= 7:\n",
    "            return '6.5-7.0'\n",
    "        elif 7 <= value <= 7.5:\n",
    "            return '7.0-7.5'\n",
    "        elif 7.5 <= value <= 8.0:\n",
    "            return '7.5-8.0'\n",
    "        elif 8.0 <= value <= 8.5:\n",
    "            return '8.0-8.5'\n",
    "        else:\n",
    "            return '8.5+'\n",
    "    else:\n",
    "        return 'NA'  # or any other label you want to assign to missing values\n",
    "\n",
    "\n",
    "def map_side_bucket(value):\n",
    "    if pd.notna(value):\n",
    "        if value < -5:\n",
    "            return '<-5.0'\n",
    "        elif -5 <= value <= -4.5:\n",
    "            return '-5.0 - -4.5'\n",
    "        elif -4.5 < value <= -4:\n",
    "            return '-4.5 - -4.0'\n",
    "        elif -4 < value <= -3.5:\n",
    "            return '-3.5 - -3.0'\n",
    "        elif -3.5 < value <= -3:\n",
    "            return '-3.0 - -2.5'\n",
    "        elif -2.5 < value <= -2:\n",
    "            return '-2.5 - -2.0'\n",
    "        elif -2 < value <= -1.5:\n",
    "            return '-2.0 - -1.5'\n",
    "        elif -1.5 < value <= -1:\n",
    "            return '-1.5 - -1.0'\n",
    "        elif -1 < value <= -0.5:\n",
    "            return '-1.0 - -0.5'\n",
    "        elif -0.5 < value <= 0:\n",
    "            return '-0.5 - 0.0'\n",
    "        elif 0 < value <= 0.5:\n",
    "            return '0.0 - 0.5'\n",
    "        elif 0.5 < value <= 1:\n",
    "            return '0.5 - 1.0'\n",
    "        elif 1 < value <= 1.5:\n",
    "            return '1.0 - 1.5'\n",
    "        elif 1.5 <= value <= 2.0:\n",
    "            return '1.5 - 2.0'\n",
    "        elif 2.0 <= value <= 2.5:\n",
    "            return '2.0 - 2.5'\n",
    "        elif 2.5 <= value <= 3.0:\n",
    "            return '2.5 - 3.0'\n",
    "        elif 3.0 <= value <= 3.5:\n",
    "            return '3.0 - 3.5'\n",
    "        elif 3.5 <= value <= 4.0:\n",
    "            return '3.5-4.0'\n",
    "        elif 4.0 <= value <= 4.5:\n",
    "            return '4.0-4.5'\n",
    "        elif 4.5 <= value <= 5.0:\n",
    "            return '4.5-5.0'\n",
    "        else:\n",
    "            return '5.0+'\n",
    "    else:\n",
    "        return 'NA'  # or any other label you want to assign to missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b65c43",
   "metadata": {},
   "source": [
    "# This is a function to add all the other functions created prior, to the pitch by pich data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14725734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_savant(df):\n",
    "    # Modify 'pfx_x' and 'pfx_z' columns to get them in inches \n",
    "    df['pfx_x'] = df['pfx_x'].apply(lambda x: x * -12)\n",
    "    df['pfx_z'] = df['pfx_z'].apply(lambda x: x * 12)\n",
    "    \n",
    "    # Create 'hard_hit' column\n",
    "    df['hard_hit'] = 0  # Initialize all values to 0\n",
    "    mask = df['launch_speed'].notna()  # Get a boolean mask of non-missing values in 'launch_speed'\n",
    "    df.loc[mask & (df['launch_speed'] > 95), 'hard_hit'] = 1  # Set values to 1 where condition is met\n",
    "    \n",
    "    # Create 'sweet_spot' column\n",
    "    df['sweet_spot'] = 0  # Initialize all values to 0\n",
    "    mask = df['launch_angle'].notna()  # Get a boolean mask of non-missing values in 'launch_angle'\n",
    "    df.loc[mask & ((df['launch_angle'] >= 8) & (df['launch_angle'] <= 32)), 'sweet_spot'] = 1  # Set values to 1 where condition is met\n",
    "\n",
    "    # Add 'VAA' column\n",
    "    vy_f = -1 * np.sqrt(df['vx0']**2 + df['vy0']**2) \n",
    "    t = (vy_f - df['vy0']) / df['ay']\n",
    "    vz_f = df['vz0'] + df['az'] * t\n",
    "    df['VAA'] = np.round(-1 * np.arctan(vz_f / vy_f) * (180 / np.pi), 2)\n",
    "    \n",
    "    # Add 'Count' column\n",
    "    df['count'] = df['balls'].astype(str) + '-' + df['strikes'].astype(str)\n",
    "\n",
    "    # Add 'count_type' column  \n",
    "    df['count_type'] = ''\n",
    "    df.loc[df['count'].isin(['1-0', '2-0', '3-0', '2-1', '3-1', '3-2']), 'count_type'] = 'hitter'\n",
    "    df.loc[df['count'].isin(['0-1', '0-2', '1-2']), 'count_type'] = 'pitcher'\n",
    "    df.loc[df['count'].isin(['0-0', '1-1', '2-2']), 'count_type'] = 'even'  \n",
    "\n",
    "    # Add a column, 'swing' to determine if the pitch was swung a or not (1=swing) \n",
    "    #this classification includes all bunt attempts as swings so you will need to filter those out in later analysis\n",
    "    #define the strings to look for\n",
    "    target_strings = ['hit_into_play', 'foul', 'swinging_strike', 'swinging_strike_blocked', 'foul_bunt', 'foul_tip', 'foul_pitchout', 'missed_bunt', 'bunt_foul_tip', 'swinging_pitchout']\n",
    "    # create a new column with 1s and 0s based on whether the target strings are present in column1\n",
    "    df['swing'] = np.where(df['description'].isin(target_strings), 1, 0)  \n",
    "\n",
    "    # Create a new column 'swing_type'\n",
    "    swing_types = df['description'].copy()\n",
    "    \n",
    "    # Determine if a pitch resulted in: contact, foul, whiff, take_ball, take_strike, or undef (undefined)\n",
    "    swing_types.loc[swing_types.isin(['hit_into_play'])] = 'contact'\n",
    "    swing_types.loc[swing_types.isin(['foul', 'foul_bunt', 'foul_tip', 'bunt_foul_tip', 'foul_pitchout'])] = 'foul'\n",
    "    swing_types.loc[swing_types.isin(['swinging_strike', 'swinging_strike_blocked', 'missed_bunt', 'swinging_pitchout'])] = 'whiff'\n",
    "    swing_types.loc[swing_types.isin(['ball', 'blocked_ball', 'hit_by_pitch', 'pitchout'])] = 'take_ball'\n",
    "    swing_types.loc[swing_types.isin(['called_strike'])] = 'take_strike'\n",
    "    swing_types.loc[~swing_types.isin(['contact', 'foul', 'whiff', 'take_ball', 'take_strike'])] = 'undef'\n",
    "\n",
    "    # Add the new column to the dataframe\n",
    "    df['swing_type'] = swing_types  \n",
    "\n",
    "    # Create a column called attack_zone (THANKS TO NICK WAN FOR THIS, his KAGGLE competition notebook helped out here)\n",
    "    df['attack_zone'] = 'waste'\n",
    "    df.loc[(df['plate_x'].between(-0.558, 0.558)) & (df['plate_z'].between(1.833,3.166)), 'attack_zone'] = 'heart'\n",
    "    df.loc[(df['plate_x'].between(-1.108, 1.108)) & (df['plate_z'].between(1.166,3.833)) & (~df['attack_zone'].isin(['heart'])), 'attack_zone'] = 'shadow'\n",
    "    df.loc[(df['plate_x'].between(-1.666, 1.666)) & (df['plate_z'].between(0.5,4.5)) & (~df['attack_zone'].isin(['heart', 'shadow'])), 'attack_zone'] = 'chase'\n",
    "    \n",
    "    # Calculate the average delta_run_exp for each player and pitch type\n",
    "    df['RV'] = df.groupby(['player_name', 'pitch_type'])['delta_run_exp'].transform('mean')\n",
    "    df['RV/100'] = df['RV'] * 100\n",
    "\n",
    "    # Add 'Rel_height_bucket' and 'Rel_side_bucket' column\n",
    "    df['Rel_height_bucket'] = df['release_pos_z'].apply(map_height_bucket)\n",
    "    df['Rel_side_bucket'] = df['release_pos_x'].apply(map_side_bucket)\n",
    "    \n",
    "    #Add 'barrel' designation to a batted ball\n",
    "    df = code_barrel(df)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4177e1b3",
   "metadata": {},
   "source": [
    "# These are the column names I will be changing to make querying quicker for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5393b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary with the column name changes\n",
    "column_names_dict = {\n",
    "    'release_speed': 'velo',\n",
    "    'release_pos_x': 'release_side',\n",
    "    'release_pos_z': 'release_height',\n",
    "    'game_date': 'date',\n",
    "    'pfx_z': 'ind_vert_break',\n",
    "    'pfx_x': 'horizontal_break',\n",
    "    'launch_speed': 'exit_velo',\n",
    "    \n",
    "    # Add more column names as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10923647",
   "metadata": {},
   "source": [
    "# Function to apply column name changes to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "414198e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_statcast_columns(data, column_names_dict):\n",
    "    \"\"\"\n",
    "    Rename columns from the Statcast data that was pulled from Savant\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Statcast data from Savant\n",
    "        column_names_dict (dict): a dictionary containing the current column names as keys\n",
    "                                  and the new column names as values\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: the modified DataFrame with the renamed columns\n",
    "    \"\"\"\n",
    "    data = data.rename(columns=column_names_dict)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2b66849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the downloaded data from pybaseball for the day through the 'add_to_savant' function\n",
    "modified_data = add_to_savant(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "418eccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the new dataframe (modified_data) that contains the new columns from the 'add_to_savant' function through \n",
    "# the function that renames the columns\n",
    "data_cleaned = rename_statcast_columns(modified_data, column_names_dict) \n",
    "#print(data_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff05e9d7",
   "metadata": {},
   "source": [
    "# We want to clear out any data in the database table before we load anything new. This is to ensure that if Statcast makes any changes we will always have the most up to date information - thanks to Jeremy Maschino for this suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1143161c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = sqlite3.connect('2023Statcast.db')\n",
    "\n",
    "# Create a cursor object to execute SQL statements\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the DROP TABLE statement to remove the entire table\n",
    "cur.execute('DROP TABLE statcast_data_2023')\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and database connections\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c1f32",
   "metadata": {},
   "source": [
    "# We will now put all of our updated data into the table 'statcast_data_2023' into the '2023Statcast' database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20f2609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to the SQLLite database 2023Statcast\n",
    "conn = sqlite3.connect('2023Statcast.db')\n",
    "\n",
    "#define the table name\n",
    "table_name = 'statcast_data_2023'\n",
    "#if the table name exists then append the data on it\n",
    "data_cleaned.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "75a43a5db8596c29e85eaed3ed89461b46b7e99833f5cf18b6dda868c2f42b7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
